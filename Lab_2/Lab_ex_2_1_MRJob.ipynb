{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Lab_ex_2_1_MRJob.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX96vJIb_Zlb"
      },
      "source": [
        "# Ex 2.1 Hadoop MapReduce with Python\n",
        "There are two prominent *Python* APIs for interfacing *Hadoop MapReduce* clusters:\n",
        "\n",
        "## *Snakebite* for *HDFS* access\n",
        "The [Snakebite Lib](https://github.com/spotify/snakebite) allows easy access to *HDFS* file systems:  \n",
        "```\n",
        ">>> from snakebite.client import Client\n",
        ">>> client = Client(\"localhost\", 8020, use_trash=False)\n",
        ">>> for x in client.ls(['/']):\n",
        "...     print x\n",
        "```\n",
        "\n",
        "See [documentation](https://snakebite.readthedocs.io/en/latest/) for details.\n",
        "\n",
        "\n",
        "## *MRJOB* for *MapReduce* job execution\n",
        "The ``mrjob`` lib -> [see docu](https://mrjob.readthedocs.io/en/latest/index.html) is a power full *MapReduce* client for *Python*. Some of the key features are:\n",
        "\n",
        "* local emulation (single and multi-core) a *Hadoop* cluster for development and debugging\n",
        "* simple access, authentication and file transfer to *Hadoop* clusters\n",
        "* powerful API for common cloud services, such as AWS or Azure   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21AiD8a8_Zli"
      },
      "source": [
        "### Preparing our environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5AC2GJp_Zlm",
        "outputId": "291c50d9-5a58-415e-dd7f-613583876e7d"
      },
      "source": [
        "#install mrjob lib and boto3 for AWS S3 access\n",
        "#!conda install -c conda-forge -y mrjob boto3\n",
        "\n",
        "!pip install mrjob boto3"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mrjob in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.87)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from mrjob) (3.13)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.87 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.87)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.87->boto3) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.87->boto3) (1.26.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.87->boto3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAqxDwCM_Zlp"
      },
      "source": [
        "## A *MRJOB* Example: WordCount (again)\n",
        "Since *Hadoop* works only on file in- and outputs, we do not have usual function based API. We need to pass our code (implementation of *Map* and *Reduce*) as executable *Python* scripts:\n",
        "\n",
        "* use *Jupyter's* ``%%file`` magic command to write the cell to file\n",
        "* create a executable script with ``__main__`` method\n",
        "* inherit from the ``MRJob`` class\n",
        "* implement ``mapper()`` and ``reducer()`` methods\n",
        "* call ``run()`` at start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk5pQMhm_Zlr",
        "outputId": "b4bac2b3-4d6c-4baf-e8b3-8a8c6b3e148c"
      },
      "source": [
        "%%file wordcount.py \n",
        "#this will save this cell as file\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordCount(MRJob):\n",
        "    def mapper(self, _, line):\n",
        "        for word in line.split():\n",
        "            yield(word, 1)\n",
        " \n",
        "    def reducer(self, word, counts):\n",
        "        yield(word, sum(counts))\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    MRWordCount.run()\n",
        "            "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting wordcount.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIgis7ij_Zlu"
      },
      "source": [
        "### execute script from cmd\n",
        "* ``-r local`` causes local multi-core emulation a *Hadoop* cluster.\n",
        "* Input files are cmd arguments\n",
        "* define ouput-file (see docs) or use streams: `` > out.txt``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99PtT6iR_Zlw",
        "outputId": "36304d77-a25f-42cf-b0c7-8e9d07235c64"
      },
      "source": [
        "! python wordcount.py -r local *.rst > out.txt"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for local runner\n",
            "Creating temp directory /tmp/wordcount.root.20210604.124808.772587\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/wordcount.root.20210604.124808.772587/output\n",
            "Streaming final output from /tmp/wordcount.root.20210604.124808.772587/output...\n",
            "Removing temp directory /tmp/wordcount.root.20210604.124808.772587...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcpos-sE_Zlx"
      },
      "source": [
        " -> results in **out.txt** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqGRZylt_Zlz"
      },
      "source": [
        "## Execution on AWS EMR\n",
        "AWS EMR is a clound formation service which allows you to create *Hadoop*, *Spark* and other data analytics clusters with a few clicks.\n",
        "\n",
        "**NOTE**: we are not endorsing AWS specifically, other cloud service providers have similar offers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BID8iqu0_Zl4"
      },
      "source": [
        "### Connect to existing cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ3A_l4A_Zl6",
        "outputId": "8e0f584d-487f-4874-f8a5-8956808add09"
      },
      "source": [
        "%%file mrjob_cluster.conf\n",
        "runners:\n",
        "  emr:\n",
        "    aws_access_key_id: YOUR_KEY_ID\n",
        "    aws_secret_access_key: YOUR_KEY_SECRET\n",
        "    region: eu-west-1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting mrjob_cluster.conf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DLcE74q_Zl9"
      },
      "source": [
        "We need the **ID** of the cluster we want to connect to - here pre-set to our Cluster today"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yl0K1dX_Zl_",
        "outputId": "04446fbd-48eb-48d5-fbea-a3f4377c8ddf"
      },
      "source": [
        "! python wordcount.py -r emr --cluster-id=j-L1BO0NYZIYY0 text1.rst text2.rst -c mrjob_cluster.conf  "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"wordcount.py\", line 14, in <module>\n",
            "    MRWordCount.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 616, in run\n",
            "    cls().execute()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 687, in execute\n",
            "    self.run_job()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 634, in run_job\n",
            "    with self.make_runner() as runner:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 713, in make_runner\n",
            "    return self._runner_class()(**self._runner_kwargs())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/emr.py\", line 358, in __init__\n",
            "    self._fix_s3_tmp_and_log_uri_opts()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/emr.py\", line 599, in _fix_s3_tmp_and_log_uri_opts\n",
            "    self._set_cloud_tmp_dir()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/emr.py\", line 617, in _set_cloud_tmp_dir\n",
            "    for bucket_name in self.fs.s3.get_all_bucket_names():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/fs/s3.py\", line 332, in get_all_bucket_names\n",
            "    return [b['Name'] for b in c.list_buckets()['Buckets']]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/retry.py\", line 108, in call_and_maybe_retry\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/botocore/client.py\", line 386, in _api_call\n",
            "    return self._make_api_call(operation_name, kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/botocore/client.py\", line 705, in _make_api_call\n",
            "    raise error_class(parsed_response, operation_name)\n",
            "botocore.exceptions.ClientError: An error occurred (InvalidAccessKeyId) when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGSCYqtX_ZmB"
      },
      "source": [
        "## Exercise\n",
        "Use  *mrjob*  to  compute  employee  **top  annual  salaries** and  **gross pay** in the *CSV* table ``Baltimore_City_employee_Salaries_FY2014.csv``.\n",
        "\n",
        "* use  ``import csv`` to read the data -> [API docs](https://docs.python.org/3/library/csv.html)\n",
        "* use ``yield`` to return *producers* from *map* and *reduce* functions\n",
        "* return top entries in both categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380hD0Df_ZmC"
      },
      "source": [
        "%%file coputeSalary.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "class ComputeSalariesandGrossPay(MRJob):\n",
        "  def mapper(self, _, line):\n",
        "    for key in line.split():\n",
        "      for row in header:\n",
        "        yield(row['AnnualSalary'], row['GrossPay'])\n",
        "\n",
        "  def reducer(self,key,value):\n",
        "    yield(key, max(value))\n",
        "    \n",
        "import csv\n",
        "with open('Baltimore_City_Employee_Salaries_FY2014.csv', newline='') as csvfile:\n",
        "  #fieldnames = ['first_name','last_name']\n",
        "  reader = csv.DictReader(csvfile)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  ComputeSalariesAndGrossPay.run()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkxhapC8JldH"
      },
      "source": [
        "!python computeSalary.py -r local > out2.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}